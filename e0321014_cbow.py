# -*- coding: utf-8 -*-
"""E0321014_CBOW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ArqbdKWhUOJF35SF3FAIk7tiI9RpNgRj
"""

import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt

nltk.download("punkt")

nltk.data.path.append('.')

import re
import jieba

path = "sample.txt"

with open(path, 'r', encoding='utf-8') as file:
    data = file.read()

data = re.sub(r'[，。！？；]', '。', data)

data_tokens = jieba.cut(data)

data_tokens = [token.lower() for token in data_tokens if token.isalpha() or token == '。']

print(data_tokens[:10])

print(f"The Number of tokens = {len(data_tokens)} \n {data[:20]}")

fdist = nltk.FreqDist(word for word in data_tokens)
print("Size of vocabulary: ",len(fdist) )
print("Most frequent tokens: ",fdist.most_common(20) )

import jieba

def get_dict(data_tokens):
    words = sorted(list(set(data_tokens)))
    word2Ind = {word: idx for idx, word in enumerate(words)}
    Ind2word = {idx: word for idx, word in enumerate(words)}
    return word2Ind, Ind2word

path = "sample.txt"

with open(path, 'r', encoding='utf-8') as file:
    data = file.read()

data_tokens = list(jieba.cut(data))

word2Ind, Ind2word = get_dict(data_tokens)

V = len(word2Ind)
print("Size of vocabulary: ", V)

print("Index of '在' in vocabulary: ", word2Ind.get('在'))
print("Word corresponding to index 0: ", Ind2word.get(68))


print("Index of the word '中国' :  ",word2Ind['中国'] )
print("Word which has index 2743:  ",Ind2word[68] )

import numpy as np

def init_parameters(N: int, V: int, random_seed: int = 1) -> dict:
    np.random.seed(random_seed)
    parameters = {}

    # Assuming V represents the vocabulary size in Mandarin (characters or tokens)
    parameters["W1"] = np.random.rand(N, V)
    parameters["b1"] = np.zeros(shape=(N, 1))

    # W2 would typically map from the hidden layer (N) back to the vocabulary size (V)
    parameters["W2"] = np.random.rand(V, N)
    parameters["b2"] = np.zeros(shape=(V, 1))

    return parameters

tmp_N = 4
tmp_V = 10
params = init_parameters(tmp_N,tmp_V)
assert params['W1'].shape == ((tmp_N,tmp_V))
assert params['W2'].shape == ((tmp_V,tmp_N))
print(f"W1 shape: {params['W1'].shape}")
print(f"W2 shape: {params['W2'].shape}")
print(f"b1 shape: {params['b1'].shape}")
print(f"b2 shape: {params['b2'].shape}")

from typing import List, Union

def softmax(z: Union[float, List[float]]) -> Union[float, List[float]]:
    y = np.exp(z) / np.sum(np.exp(z), 0, keepdims=True)

    return y

def sigmoid(z: Union[float, List[float]]) -> Union[float, List[float]]:
    return 1.0 / (1.0 + np.exp(-z))

def relu(z: Union[float, List[float]]) -> Union[float, List[float]]:
    return np.maximum(z, 0)

tmp = np.array([[1,2,3],
                [1,1,1]
               ])
tmp_sm = softmax(tmp)
display(tmp_sm)

def forward_propagation(x: np.ndarray, params: dict) -> tuple:
    z1 = np.dot(params['W1'], x) + params['b1']
    h = relu(z1)

    z2 = np.dot(params['W2'], h) + params['b2']
    y_hat = z2

    return y_hat, h

tmp_x = np.array([[0,1,0,0,0,0,0,0,0,0]]).T
tmp_z, tmp_h = forward_propagation(tmp_x, params)
print("call forward_prop")
print()
# Look at output
print(f"z has shape {tmp_z.shape}")
print("z has values:")
print(tmp_z)

def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray,
                       batch_size: int) -> Union[float, List[float]]:
    logprobs = np.multiply(np.log(y_hat), y)
    cost = -1/batch_size * np.sum(logprobs)
    cost = np.squeeze(cost)
    return cost



def backward_propagation(x: np.ndarray, y: np.ndarray, y_hat: np.ndarray, h: np.ndarray,
                         params: dict, batch_size: int) -> dict:
    grads_params = {}

    l1 = np.dot(params['W2'].T ,(y_hat - y))

    grads_params['W1'] = np.dot(l1, x.T) / batch_size
    grads_params['W2'] = np.dot((y_hat - y), h.T) / batch_size

    grads_params['b1'] = np.sum(l1, axis=1, keepdims= True) / batch_size
    grads_params['b2'] = np.sum((y_hat - y), axis= 1, keepdims= True) / batch_size

    return grads_params

from collections import defaultdict
import numpy as np
import jieba

def get_idx(words, word2Ind):
    idx = []
    for word in words:
        idx.append(word2Ind[word])
    return idx

def pack_idx_with_frequency(context_words, word2Ind):
    freq_dict = defaultdict(int)
    for word in context_words:
        freq_dict[word] += 1
    idxs = get_idx(context_words, word2Ind)
    packed = []
    for i in range(len(idxs)):
        idx = idxs[i]
        freq = freq_dict[context_words[i]]
        packed.append((idx, freq))
    return packed

def get_vectors(data, word2Ind, V, C):
    i = C
    while True:
        y = np.zeros(V)
        x = np.zeros(V)
        center_word = data[i]
        y[word2Ind[center_word]] = 1
        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]
        num_ctx_words = len(context_words)
        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):
            x[idx] = freq / num_ctx_words
        yield x, y
        i += 1
        if i >= len(data) - C:
            print("i is being set to", C)
            i = C

def get_batches(data, word2Ind, V, C, batch_size):
    batch_x = []
    batch_y = []
    for x, y in get_vectors(data, word2Ind, V, C):
        if len(batch_x) < batch_size:
            batch_x.append(x)
            batch_y.append(y)
        else:
            yield np.array(batch_x).T, np.array(batch_y).T
            batch_x = []
            batch_y = []

import numpy as np

def gradient_descent(data: list[str], word2Ind: dict, N: int,
                     V: int, epochs: int, alpha: float = 0.03,
                     random_seed: int = 282) -> dict:

    param = init_parameters(N, V, random_seed=random_seed)

    batch_size = 128
    epoch = 0
    C = 2

    for x, y in get_batches(data, word2Ind, V, C, batch_size):

        z, h = forward_propagation(x, param)
        yhat = softmax(z)

        cost = cross_entropy_loss(yhat, y, batch_size)
        if (epoch + 1) % 10 == 0:
            print(f"Iteration: {epoch + 1} Cost: {cost:.6f}")

        grads = backward_propagation(x, y, yhat, h, param, batch_size)

        param['W1'] -= alpha * grads['W1']
        param['W2'] -= alpha * grads['W2']
        param['b1'] -= alpha * grads['b1']
        param['b2'] -= alpha * grads['b2']

        epoch += 1
        if epoch == epochs:
            break
        if epoch % 100 == 0:
            alpha *= 0.66

    return param



C = 2
N = 50
word2Ind, Ind2word = get_dict(data)
V = len(word2Ind)
num_iters = 150
print("Call gradient_descent")
params = gradient_descent(data, word2Ind, N, V, num_iters)

